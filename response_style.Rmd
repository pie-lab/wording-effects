---
title: "Does item format impact response syle?"
date: "Last updated `r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: TRUE
    toc_float: TRUE
---

The primary aims of this study are to evaluate the effects of item wording in online, self-report personality assessment. Specifically, we intend to consider the extent to which incremental wording changes may influence differences in participant response style. These wording changes will include a progression from using (1) trait-descriptive adjectives by themselves, (2) with the linking verb “to be” (Am…), (3) with the additional verb “to tend” (Tend to be...), and (4) with the pronoun “someone” (Am someone who tends to be…). 

In this section, we test the impact of item format on three components of response style:

  1. Expected (average) response
  2. Likelihood of extreme responding
  3. Nay-saying
  
For these analyses, we use data from Blocks 1 and 2.

As a reminder, the (numeric) range of options for items was 1-6. Some items are reverse-scored. Those items are `r reverse`. For the majority of the analyses in this section, we use only the items included in the MIDI scales (i.e., we exclude items included from the Big Five Mini Markers -- these are only tested in analyses related to yea-saying, below).

## Deviations from preregistration

<!-- We swapped out the function `aov` to calculate the significance of the categorical factor to `car::Anova` -- this changes the statistical test from an F-test to a Likelihood ratio test (i.e., comparing the model with to the model without the categorical factor). We made this choice for two reasons: (1) We found that the `aov` function was not test the significance of interactions, and (2) the significance test from the `aov` function was inconsistent with plotted means (e.g., the test was significant, but the means were not close together and confidence interval were highly overlapping). In general, we found the `car::Anova` function to be both more interpretable and more conservative. Data are made availale for interested readers to use their own tests of significance. -->

We switched out our plotting function from using the `sjPlot` package to using the `marginaleffects` package -- to calculated the average predicted value for each group -- and plotting those using `ggplot2.` We found that these estimates better accounted for the sample size and nesting in the multilevel models.


```{r responsestyle1, include = FALSE}
knitr::opts_chunk$set(warning = F, message = F)
options(scipen = 999)
```

```{r responsestyle2, echo = F}
library(here) # for working with files
library(tidyverse) # for cleaning
library(janitor) # for variable names
library(glmmTMB) # for binomial models
library(marginaleffects) # for comparisons
library(car) # for correct sums of squares
library(sjPlot) # for figures
library(ggpubr) # for prettier plots
library(kableExtra) # for nicer tables
library(broom.mixed) # for tidying multilevel models
library(papaja) # pretty numbers
```

```{r responsestyle3, echo = F}
load(here("objects/items_df.Rds"))
load(here("objects/bfmm.Rds"))
load(here("objects/reverse_vector.Rds"))
```

## Expected response

We used a multilevel model. Our primary predictor was format. We use data from all three blocks; as a consequence, each person contributes either two or three data points for each of the trait descriptive adjectives. Thus, we nest responses within participant to account for this dependency. This is equivalent to a repeated measures ANOVA. However, in this omnibus model, we include responses to all trait adjectives. Thus, we must also account for adjective-specific contributions to variability. Finally, we include a random term for block. This is not hypothesized to account for significant variability, but we include this term in the event that block contributes significantly to ratings.

We use the `aov` function to calculate the amount of variability in response due to format. 

```{r responsestyle4 }
mod.expected = items_df %>% 
  filter(block %in% c(1,2)) %>% 
  filter(!(item %in% bfmm)) %>% 
  glmmTMB(response~format + (1|item) + (1|proid) + (1|block), 
                  data = .)

tidy(aov(mod.expected))
```

```{r responsestyle5, echo = F}
fb1_aov = tidy(aov(mod.expected))
fb1_loc = which(fb1_aov$term == "format")
fb1_res = which(fb1_aov$term == "Residuals")
fb1_sig = fb1_aov$p.value[fb1_loc] < .05
fb1_aov = fb1_aov %>% 
  mutate(across(starts_with("P"), papaja::printp),
         across(where(is.numeric), papaja::printnum))
```

Item format was `r ifelse(fb1_sig, "associated", "unassociated")` with participants’ expected responses to personality items $(F(`r fb1_aov$df[[1]]`, `r fb1_aov$df[[fb1_res]]`) = `r fb1_aov$statistic[[fb1_loc]]`, p = `r fb1_aov$p.value[[fb1_loc]]`)$. See Figure \@ref(fig:responsestyle7) for a visualization of this effect. In addition, Figure \@ref(fig:responsestyle8) shows the full distribution of responses across format. We note too that expected responses varied as a function of item $(F(`r fb1_aov$df[[2]]`, `r fb1_aov$df[[fb1_res]]`) = `r fb1_aov$statistic[[2]]`, p = `r fb1_aov$p.value[[2]]`)$ but not block $(F(`r fb1_aov$df[[4]]`, `r fb1_aov$df[[fb1_res]]`) = `r fb1_aov$statistic[[4]]`, p = `r fb1_aov$p.value[[4]]`)$.

```{r responsestyle6, echo = F}
# save for power analysis
save(mod.expected, file = here("objects/mod_expected.Rdata"))
```


```{r responsestyle7, fig.cap = "Predicted response on personality items by condition.", echo = F}
pred.expected = predictions(mod.expected, by = "format", type = "response") 

plot_expected = pred.expected %>% 
  ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .5) +
  labs(title = "Expected response",
       x = NULL, 
       y = NULL) + theme_minimal(base_size = 15)

plot_expected
```

```{r responsestyle8, fig.cap = "Distribution of responses by category.", echo = F}
means_by_group = items_df %>% 
  filter(block %in% c(1,2)) %>% 
  filter(!(item %in% bfmm)) %>% 
  group_by(format) %>%
  summarise(m = mean(response),
            s = sd(response))

counts_by_group = items_df %>%
  group_by(format, response) %>%
  count()

items_df %>%
  ggplot(aes(x = response, fill = format)) +
  geom_histogram(bins = 6, color = "white") +
  geom_vline(aes(xintercept = m), data = means_by_group) +
  geom_text(aes(x = 1,
                y = max(counts_by_group$n),
                label = paste("M =", round(m,2),
                              "\nSD =", round(s,2))),
            data = means_by_group,
            hjust =0,
            vjust = 1) +
  facet_wrap(~format) +
  guides(fill = "none") +
  scale_x_continuous(breaks = 1:6) +
  labs(y = "Number of particpants",
       title = "Distribution of responses by format") +
  theme_pubr()
```

### One model for each adjective

We repeat this analysis separately for each trait. 

```{r responsestyle9, results = 'asis'}
mod_by_item = items_df %>%
  filter(block %in% c(1,2)) %>% 
  filter(!(item %in% bfmm)) %>% 
  group_by(item) %>%
  nest() %>%
  mutate(mod = map(data, ~glmmTMB(response~format + (1|proid) + (1|block), 
                                  data = .))) %>%
  mutate(aov = map(mod, aov))
```

We apply a Holm correction to the _p_-values extracted from these analyses, to adjust for the number of tests conducted. We present results in Table \@ref(tab:responsestyle10), which is organized by whether items were reverse-coded prior to analysis.

```{r responsestyle10, echo = F, results = 'asis'}
summary_by_item = mod_by_item %>%
  ungroup() %>%
  mutate(tidy = map(aov, broom::tidy)) %>%
  select(item, tidy) %>%
  unnest(cols = c(tidy)) %>%
  mutate(df2 = ifelse(term == "Residuals", df, NA)) %>% 
  with_groups(item, fill, df2, .direction = "downup") %>% 
  filter(term == "format") %>% 
  mutate(reverse = case_when(
    item %in% reverse ~ "Y",
    TRUE ~ "N"
  )) %>%
  mutate(p.adj = p.adjust(p.value, method = "holm"))

summary_by_item %>%
  arrange(reverse, item) %>%
  mutate(across( starts_with("p"), printp )) %>% # format p-values
  select(item, reverse, sumsq, meansq, df, df2, statistic, p.value, p.adj) %>% 
  kable(digits = 2,
        booktabs = T, 
        align = c("l", rep("c", 6), rep("r",2)),
        col.names = c("Item", "Reverse\nScored?", "SS", "MS", "df1", "df2", "F", "raw", "adj"),
        caption = "Format effects on expected response by item.") %>%
  kable_styling()
```

### Pairwise t-tests for significant ANOVAs

When format was a significant predictor of expected response for an item (using the un-adjusted _p_-value here), we follow up with pairwise comparisons of format. Here we identify the items which meet this criteria. In the manuscript proper, we will only report the results for items in which format was significant, even after applying the Holm correction.

Differences in means and significance are shown in Table \@ref(tab:responsestyle12). These are also plotted in Figure \@ref(fig:responsestyle13).

```{r responsestyle11 }
sig_item = summary_by_item %>%
  filter(p.value < .05)

sig_item = sig_item$item
sig_item
```

```{r}
pairwise_response = mod_by_item %>% 
  #only significant items
  filter(item %in% sig_item) %>% 
  #use marginaleffects package to calculate format means and run pairwise comparisons
  mutate(
    means = map(mod, 
                avg_predictions, 
                variables = "format"),
    comp = map(mod, 
               avg_comparisons, 
               variables = list(format = "pairwise")))
```

```{r responsestyle12}
pairwise_response %>% 
  select(item, comp) %>% 
  unnest(cols = c(comp)) %>% 
  mutate(estimate = printnum(estimate),
         estimate = case_when(
           p.value < .001 ~ paste0(estimate, "***"),
           p.value < .01 ~ paste0(estimate, "**"),
           p.value < .05 ~ paste0(estimate, "*"),
           TRUE ~ estimate
         )) %>% 
  mutate(
    contrast = str_replace(contrast, "Adjective\nOnly", "A"),
    contrast = str_replace(contrast, "Am\nAdjective", "B"),
    contrast = str_replace(contrast, "Tend to be\nAdjective", "C"),
    contrast = str_replace(contrast, "Am someone\nwho tends to be\nAdjective", "D"),
    contrast = str_remove_all(contrast, " ")
  ) %>% 
  select(item, contrast, estimate) %>% 
  pivot_wider(names_from = contrast, values_from = estimate) %>% 
  kable(booktabs = T,
        caption = "Pairwise differences of means by format. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective. * p < .05, ** p < .01, *** p < .001") %>%
  kable_styling()
```

```{r responsestyle13, fig.cap = "Expected means by format and item. These items were significantly affected by response. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective." }
pairwise_response %>% 
  select(item, means) %>% 
  unnest(cols = c(means)) %>% 
  mutate(format = case_when(
    format == "Adjective\nOnly" ~ 1,
    format == "Am\nAdjective" ~ 2,
    format == "Tend to be\nAdjective" ~ 3,
    format == "Am someone\nwho tends to be\nAdjective" ~ 4)) %>% 
  ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity") + 
  geom_line(alpha = .3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .3) +
  scale_x_continuous(breaks = c(1:4), labels= c("A","B","C","D")) +
  labs(x = NULL, y = "Expected response") +
  facet_wrap(~item) +
  theme_pubr() 
```

## Extreme responding

We define _extreme responding_ as answering either a 1 (Very inaccurate) or a 6 (Very accurate). To model likelihood of extreme responding by format, we use logistic regression.

```{r responsestyle14}
items_df = items_df %>% 
  mutate(extreme = case_when(
    response == 1 ~ 1,
    response == 6 ~ 1,
    TRUE ~ 0
  ))
```

```{r responsestyle15}
mod.extreme = items_df %>% 
  filter(block %in% c(1,2)) %>% 
  filter(!(item %in% bfmm)) %>% 
  glmmTMB(extreme~format + (1|proid) + (1|item) + (1|block), 
          data = ., 
          family = "binomial")
tidy(aov(mod.extreme))
```

```{r responsestyle16, echo = F}
fb2_aov = tidy(aov(mod.extreme))
fb2_loc = which(fb2_aov$term == "format")
fb2_res = which(fb2_aov$term == "Residuals")
fb2_sig = fb2_aov$p.value[[fb2_loc]] < .05
fb2_aov = fb2_aov %>% 
  mutate(across(starts_with("P"), papaja::printp),
         across(where(is.numeric), papaja::printnum))
```

Item format was `r ifelse(fb2_sig, "associated", "unassociated")` with extreme responding to personality items $(F(`r fb2_aov$df[[fb2_loc]]`, `r fb2_aov$df[[fb2_res]]`) = `r fb2_aov$statistic[[fb2_loc]]`, p = `r fb2_aov$p.value[[fb2_loc]]`)$. See Figure \@ref(fig:responsestyle17) for a visualization of this effect. We note too that extreme responding varied as a function of item $(F(`r fb2_aov$df[[2]]`, `r fb2_aov$df[[fb2_res]]`) = `r fb2_aov$statistic[[2]]`, p = `r fb2_aov$p.value[[2]]`)$ and block $(F(`r fb2_aov$df[[4]]`, `r fb2_aov$df[[fb2_res]]`) = `r fb2_aov$statistic[[4]]`, p = `r fb2_aov$p.value[[4]]`)$.


```{r , echo = F}
# save for power analysis
save(mod.extreme, items_df, file = here("objects/mod_extreme.Rdata"))
```



```{r responsestyle17, fig.cap = "Predicted response on personality items by condition.", echo = F}
pred.extreme = predictions(mod.extreme, by = "format", type = "response") 

plot_extreme = pred.extreme %>% ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .5) +
  labs(title = "Likelihood of extreme responding",
       x = NULL, 
       y = "Likelihood") + theme_minimal(base_size = 15)

plot_extreme
```


### One model for each adjective

We repeat this analysis separately for each trait. 

```{r responsestyle18, results = 'asis'}
mod_by_item_ex = items_df %>%
  filter(block %in% c(1,2)) %>%
  filter(!(item %in% bfmm)) %>% 
  group_by(item) %>%
  nest() %>%
  mutate(mod = map(data, ~glmmTMB(extreme~format + (1|proid) + (1|block), 
                                  data = .,
                                  family = "binomial"))) %>%
  mutate(aov = map(mod, aov))
```

We apply a Holm correction to the _p_-values extracted from these analyses, to adjust for the number of tests conducted. We present results in Table \@ref(tab:responsestyle19), which is organized by whether items were reverse-coded prior to analysis.

```{r responsestyle19, echo = F, results = 'asis'}
summary_by_item_ex = mod_by_item_ex %>%
  ungroup() %>%
  mutate(sum = map(aov, tidy)) %>%
  select(item, sum) %>%
  unnest(cols = c(sum)) %>%
  mutate(df2 = ifelse(term == "Residuals", df, NA)) %>% 
  with_groups(item, fill, df2, .direction = "downup") %>% 
  filter(term == "format") %>% 
  mutate(reverse = case_when(
    item %in% reverse ~ "Y",
    TRUE ~ "N"
  )) %>%
  mutate(p.adj = p.adjust(p.value, method = "holm"))

summary_by_item_ex %>%
  mutate(across( starts_with("p"), printp )) %>% # format p-values
  arrange(reverse, item) %>%
  select(item, reverse, sumsq, meansq, df, df2, statistic, p.value, p.adj) %>%
  kable(digits = 2,
        booktabs = T,
        align = c("l", rep("c", 6), rep("r",2)),
        col.names = c("Item", "Reverse\nScored?", "SS", "MS", "df", "df2" , "F", "raw", "adj"),
        caption = "Format effects on extreme response by item.") %>%
  kable_styling()
```

### Pairwise t-tests for significant ANOVAs

When format was a significant predictor of extreme responding for an item (using the un-adjusted _p_-value here), we follow up with pairwise comparisons of format. Here we identify the items which meet this criteria. In the manuscript proper, we will only report the results for items in which format was significant, even after applying the Holm correction.

```{r responsestyle20 }
sig_item_ex = summary_by_item_ex %>%
  filter(p.value < .05)

sig_item_ex = sig_item_ex$item
sig_item_ex
```

Then we create models for each adjective. We use the `emmeans` package to perform pairwise comparisons, again with a Holm correction on the _p_-values. We also plot the means and 95% confidence intervals of each mean. Likelihood differences are shown in Table \@ref(tab:responsestyle23) and likelihood estimates are in Figure \@ref(fig:responsestyle24).

```{r itemextreme}
pairwise_response_ex = mod_by_item_ex %>% 
  #only significant items
  filter(item %in% sig_item_ex) %>% 
  #use marginaleffects package to calculate format means and run pairwise comparisons
  mutate(
    means = map(mod, 
                avg_predictions, 
                variables = "format", 
                type = "response"),
    comp = map(mod, 
               avg_comparisons, 
               variables = list(format = "pairwise"),
               type = "response"))
```

```{r responsestyle23 }
pairwise_response_ex %>% 
  select(item, comp) %>% 
  unnest(cols = c(comp)) %>% 
  mutate(estimate = printnum(estimate),
         estimate = case_when(
           p.value < .001 ~ paste0(estimate, "***"),
           p.value < .01 ~ paste0(estimate, "**"),
           p.value < .05 ~ paste0(estimate, "*"),
           TRUE ~ estimate
         )) %>% 
  mutate(
    contrast = str_replace(contrast, "Adjective\nOnly", "A"),
    contrast = str_replace(contrast, "Am\nAdjective", "B"),
    contrast = str_replace(contrast, "Tend to be\nAdjective", "C"),
    contrast = str_replace(contrast, "Am someone\nwho tends to be\nAdjective", "D"),
    contrast = str_remove_all(contrast, " ")
  ) %>% 
  select(item, contrast, estimate) %>% 
  pivot_wider(names_from = contrast, values_from = estimate) %>% 
  kable(booktabs = T,
        caption = "Pairwise differences in likelihood of extreme responding by format. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective. * p < .05, ** p < .01, *** p < .001") %>%
  kable_styling()
```

```{r responsestyle24, fig.cap = "Extreme responding by format and item. These items were significantly affected by response. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective." }

pairwise_response_ex %>% 
  select(item, means) %>% 
  unnest(cols = c(means)) %>% 
  mutate(format = case_when(
    format == "Adjective\nOnly" ~ 1,
    format == "Am\nAdjective" ~ 2,
    format == "Tend to be\nAdjective" ~ 3,
    format == "Am someone\nwho tends to be\nAdjective" ~ 4)) %>% 
  ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity") + 
  geom_line(alpha = .3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .3) +
  scale_x_continuous(breaks = c(1:4), labels= c("A","B","C","D")) +
  labs(x = NULL, y = "Probability of extreme response") +
  facet_wrap(~item) +
  theme_pubr() 
```


## Yea-saying

We define _yea-saying_ as answering "somewhat accurate" (4), "accurate" (5), or "very accurate" (6) to an item. To model likelihood of yea-saying  by format, we use logistic regression. As a reminder, we reverse-scored socially desirable items during the cleaning stage. For those items, responses coded as 1, 2, or 3 represent agreement (accurate). Therefore, we code values 1, 2, and 3 as yea-saying for reverse-scored items, and values 4, 5, and 6 as yea-saying for all other items.

For these analyses, we only used a set of matched pairs of adjectives to create balanced subsets of positively and negatively keyed items.

```{r responsestyle25}
items_df = items_df %>% 
  mutate(
    yeasaying = case_when(
    item %in% reverse & response %in% c(1:3) ~ 1,
    !(item %in% reverse) & response %in% c(4:6) ~ 1,
    TRUE ~ 0
  ))
```

```{r responsestyle26}
mod.yeasaying = items_df %>% 
  filter(block %in% c(1,2)) %>%
  filter(item %in% 
           c("outgoing", "shy", "talkative", "quiet",
             "sympathetic", "unsympathetic", "warm", "cold",
             "cautious", "careless", "responsible", "reckless",
             "worrying", "relaxed", "nervous", "calm",
             "creative", "uncreative", "intelligent", "unintellectual")) %>% 
  glmmTMB(yeasaying~format + (1|proid) + (1|item) + (1|block), 
                  data = ., 
                  family = "binomial")
tidy(aov(mod.yeasaying))
```

```{r responsestyle27, echo = F}
fb3_aov = tidy(aov(mod.yeasaying))
fb3_loc = which(fb3_aov$term == "format")
fb3_res = which(fb3_aov$term == "Residuals")
fb3_sig = fb3_aov$p.value[[fb3_loc]] < .05
fb3_aov = fb3_aov %>% 
  mutate(across(starts_with("P"), papaja::printp),
         across(where(is.numeric), papaja::printnum))
```

Item format was `r ifelse(fb3_sig, "associated", "unassociated")` with yea-saying $(F(`r fb3_aov$df[[fb3_loc]]`, `r fb3_aov$df[[fb3_res]]`) = `r fb3_aov$statistic[[fb3_loc]]`, p = `r fb3_aov$p.value[[fb3_loc]]`)$. See Figure \@ref(fig:responsestyle28) for a visualization of this effect. We note too that yea-saying varied as a function of item $(F(`r fb3_aov$df[[2]]`, `r fb3_aov$df[[fb3_res]]`) = `r fb3_aov$statistic[[2]]`, p = `r fb3_aov$p.value[[2]]`)$ and block $(F(`r fb3_aov$df[[4]]`, `r fb3_aov$df[[fb3_res]]`) = `r fb3_aov$statistic[[4]]`, p = `r fb3_aov$p.value[[4]]`)$.

```{r , echo = F}
# save for power analysis
save(mod.yeasaying, items_df, file = here("objects/mod_yeasaying.Rdata"))
```


```{r responsestyle28, fig.cap = "Likelihood of yea-saying to personality items by condition.", echo = F}
pred.yea = predictions(mod.yeasaying, by = "format", type = "response") 

plot_yea = pred.yea %>% ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity", size = 2) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .5) +
  labs(title = "Likelihood of acquiescent responding",
       x = NULL, 
       y = "Likelihood") + theme_minimal(base_size = 15)

plot_yea
```

### One model for each adjective

We repeat this analysis separately for each trait. 

```{r responsestyle29, results = 'asis'}
mod_by_item_ya = items_df %>%
  filter(item %in% 
           c("outgoing", "shy", "talkative", "quiet",
             "sympathetic", "unsympathetic", "warm", "cold",
             "cautious", "careless", "responsible", "reckless",
             "worrying", "relaxed", "nervous", "calm",
             "creative", "uncreative", "intelligent", "unintellectual")) %>% 
  group_by(item) %>%
  nest() %>%
  mutate(mod = map(data, ~glmmTMB(yeasaying~format + (1|proid) + (1|block), 
                                  data = .,
                                  family = "binomial"))) %>%
  mutate(aov = map(mod, aov))
```

We apply a Holm correction to the _p_-values extracted from these analyses, to adjust for the number of tests conducted. We present results in Table \@ref(tab:responsestyle30), which is organized by whether items were reverse-coded prior to analysis.

```{r responsestyle30, echo = F, results = 'asis'}
summary_by_item_ya = mod_by_item_ya %>%
  ungroup() %>%
  mutate(sum = map(aov, tidy)) %>%
  select(item, sum) %>%
  unnest(cols = c(sum)) %>%
  mutate(df2 = ifelse(term == "Residuals", df, NA)) %>% 
  with_groups(item, fill, df2, .direction = "downup") %>% 
  filter(term == "format") %>% 
  mutate(reverse = case_when(
    item %in% reverse ~ "Y",
    TRUE ~ "N"
  )) %>%
  mutate(p.adj = p.adjust(p.value, method = "holm"))

summary_by_item_ya %>%
  mutate(across( starts_with("p"), printp )) %>% # format p-values
  arrange(reverse, item) %>%
  select(item, reverse, sumsq, meansq, df, df2, statistic, p.value, p.adj) %>%
  kable(digits = 2,
        booktabs = T,
        align = c("l", rep("c", 6), rep("r",2)),
        col.names = c("Item", "Reverse\nScored?", "SS", "MS", "df", "df2", "F", "raw", "adj"),
        caption = "Format effects on yea-saying by item.") %>%
  kable_styling()
```

### Pairwise t-tests for significant ANOVAs

When format was a significant predictor of yea-saying for an item (using the un-adjusted _p_-value here), we follow up with pairwise comparisons of format. Here we identify the items which meet this criteria. In the manuscript proper, we will only report the results for items in which format was significant, even after applying the Holm correction.

```{r responsestyle31 }
sig_item_ya = summary_by_item_ya %>%
  filter(p.value < .05)

sig_item_ya = sig_item_ya$item
sig_item_ya
```

Then we create models for each adjective. We use the `marginaleffectss` package to perform pairwise comparisonss. We also plot the means and 95% confidence intervals of each mean. Likelihood differences are shown in Table \@ref(tab:responsestyle23) and likelihood estimates are in Figure \@ref(fig:responsestyle24).

```{r}
pairwise_response_ya = mod_by_item_ya %>% 
  #only significant items
  filter(item %in% sig_item_ya) %>% 
  #use marginaleffects package to calculate format means and run pairwise comparisons
  mutate(
    means = map(mod, 
                avg_predictions, 
                variables = "format", 
                type = "response"),
    comp = map(mod, 
               avg_comparisons, 
               variables = list(format = "pairwise"),
               type = "response"))
```

```{r responsestyle32 }
pairwise_response_ya %>% 
  select(item, comp) %>% 
  unnest(cols = c(comp)) %>% 
  mutate(estimate = printnum(estimate),
         estimate = case_when(
           p.value < .001 ~ paste0(estimate, "***"),
           p.value < .01 ~ paste0(estimate, "**"),
           p.value < .05 ~ paste0(estimate, "*"),
           TRUE ~ estimate
         )) %>% 
  mutate(
    contrast = str_replace(contrast, "Adjective\nOnly", "A"),
    contrast = str_replace(contrast, "Am\nAdjective", "B"),
    contrast = str_replace(contrast, "Tend to be\nAdjective", "C"),
    contrast = str_replace(contrast, "Am someone\nwho tends to be\nAdjective", "D"),
    contrast = str_remove_all(contrast, " ")
  ) %>% 
  select(item, contrast, estimate) %>% 
  pivot_wider(names_from = contrast, values_from = estimate) %>% 
  kable(booktabs = T,
        caption = "Pairwise differences in likelihood of yea-saying by format. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective. * p < .05, ** p < .01, *** p < .001") %>%
  kable_styling()
```

```{r responsestyle33, fig.cap = "Yea-saying by format and item. These items were significantly affected by response. A = Adjective only. B = Am Adjective. C = Tend to be Adjective. D = Am someone who tends to be Adjective." }
pairwise_response_ya %>% 
  select(item, means) %>% 
  unnest(cols = c(means)) %>% 
  mutate(format = case_when(
    format == "Adjective\nOnly" ~ 1,
    format == "Am\nAdjective" ~ 2,
    format == "Tend to be\nAdjective" ~ 3,
    format == "Am someone\nwho tends to be\nAdjective" ~ 4)) %>% 
  ggplot(aes(x = format, y = estimate)) +
  geom_point(stat = "identity") + 
  geom_line(alpha = .3) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .3) +
  scale_x_continuous(breaks = c(1:4), labels= c("A","B","C","D")) +
  labs(x = NULL, y = "Probability of yeasaying") +
  facet_wrap(~item) +
  theme_pubr() 
```

## All tests

```{r, echo = F, results = 'hide', fig.height = 10, fig.width = 5}
ggarrange(
  plot_expected, 
  plot_extreme,
  plot_yea, labels = c("A", "B", "C"),
  align = "hv",
  ncol = 1)
ggsave("response_style.png", height = 12, width = 6)
```

